#!/usr/bin/env python

import os
import sys
import traceback
import pickle
from sklearn.cluster import KMeans
import pandas as pd

# Paths where SageMaker mounts data in our container
prefix = '/opt/ml/'
input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model', 'recommender_system')

# This algorithm has two channels: training (mandatory) and testing (optional).
# When input method is file mode, the input files are copied to the directory
# specified here.
channel_name_train = 'train'
training_path = os.path.join(input_path, channel_name_train)
""" channel_name_test = 'test'
test_path = os.path.join(input_path, channel_name_test)
test_flag = False
 """


def load_data():
    progress_path=os.path.join(training_path,'Progress.csv')
    courses_path=os.path.join(training_path,'CPD_Courses.csv')
    df_progress = pd.read_csv(progress_path)
    df_courses = pd.read_csv(courses_path)
    
    return df_progress,df_courses

def get_list_of_categories(courses):
    category_list = []
    
    for category in courses.Categories.str.split('|'):
        for name in category:
            if name not in category_list: 
                category_list.append(name.strip())
            
    return category_list

def get_column_name_list(category_list):
    column_name = []
    
    for category in category_list:
        column_name.append('avg_' + category.strip() + '_watch')
      
    return column_name

#category watch time across ALL students across ALL categories
def get_all_category_watch_time(users, courses):
    category_progress = pd.DataFrame(columns = ['UserID'])
    category_list = get_list_of_categories(courses)
    column_names = get_column_name_list(category_list)
    
    #add studentId to list of columns
    column_names.insert(0,'UserID')
    
    for category in category_list:        
        course_categories = courses[courses['Categories'].str.contains(category)]
        
        #determine the average watch time for the given category; retain the studentId
        avg_watch_time_per_user = users[users['CourseID'].isin(course_categories['CourseID'])].loc[:, ['UserID', 'Progress']].groupby(['UserID'])['Progress'].mean().round(2).reset_index()      
    
        #merge the progress for the given catetgory with the prior categories
        category_progress = category_progress.merge(avg_watch_time_per_user, on='UserID', how='outer')
            
    category_progress.columns = column_names
    return category_progress

def dump(file_name, predictions, trainset=None, algo=None):
    """Dump a list of :obj:`predictions
    <surprise.prediction_algorithms.predictions.Prediction>` for future
    analysis, using Pickle.

    If needed, the :class:`trainset <surprise.dataset.Trainset>` object and the
    algorithm can also be dumped. What is dumped is a dictionnary with keys
    ``'predictions``, ``'trainset'``, and ``'algo'``.

    The dumped algorithm won't be a proper :class:`algorithm
    <surprise.prediction_algorithms.algo_base.AlgoBase>` object but simply a
    dictionnary with the algorithm attributes as keys-values (technically, the
    ``algo.__dict__`` attribute).

    See :ref:`User Guide <dumping>` for usage.

    Args:
        file_name(str): The name (with full path) specifying where to dump the
            predictions.

        predictions(list of :obj:`Prediction\
            <surprise.prediction_algorithms.predictions.Prediction>`): The
            predictions to dump.
        trainset(:class:`Trainset <surprise.dataset.Trainset>`, optional): The
            trainset to dump.
        algo(:class:`Algorithm\
            <surprise.prediction_algorithms.algo_base.AlgoBase>`, optional):
            algorithm to dump.
    """

    dump_obj = dict()

    dump_obj['predictions'] = predictions

    if trainset is not None:
        dump_obj['trainset'] = trainset

    if algo is not None:
        dump_obj['algo'] = algo.__dict__  # add algo attributes
        dump_obj['algo']['name'] = algo.__class__.__name__

    pickle.dump(dump_obj, open(file_name, 'wb'))
    print('The dump has been saved as file', file_name)

def train():
    print('Training job started')
    try:
        progress,courses=load_data()
        
    
        #convert progress percentage string to numeric data
        progress['Progress'] = progress['Progress'].astype('float') / 100.0

        # Calculate the average rating all categories per user
        category_watch_time_df = get_all_category_watch_time(progress, courses)
        
        #replace NaN with 0
        category_watch_time_df = category_watch_time_df.fillna(0)
        
        #remove student id from dataframe
        category_watch_time_list = category_watch_time_df.drop(['UserID'], axis=1)
        
        category_watch_time_list.shape
        
        # Turn our dataset into a list
        category_watch_time_list = category_watch_time_list.values
        
        # Create an instance of KMeans to find 20 clusters
        km = KMeans(n_clusters=20, random_state=0)

        algo=km.fit(category_watch_time_list)
        # Use fit_predict to cluster the dataset
        # Returns a cluster prediction for each student / ie cluster labels
        #predictions = algo.predict(category_watch_time_list)

        
        # Save model
        print(f"Saving model in {model_path} ...")
        
        dump.dump(model_path, algo=algo, predictions=None)
        print("Training complete.")

    except Exception as e:
        # Write out an error file. This will be returned as the reason for
        # failure in the DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)


if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
