#!/usr/bin/env python

import os
import sys
import traceback
import pickle
from scipy.sparse import csr_matrix
import pandas as pd
from sklearn.neighbors import NearestNeighbors

# Paths where SageMaker mounts data in our container
prefix = '/opt/ml/'
input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model', 'recommender_system')

# This algorithm has two channels: training (mandatory) and testing (optional).
# When input method is file mode, the input files are copied to the directory
# specified here.
channel_name_train = 'train'
training_path = os.path.join(input_path, channel_name_train)
""" channel_name_test = 'test'
test_path = os.path.join(input_path, channel_name_test)
test_flag = False
 """


def load_data():
    ratings_path=os.path.join(training_path,'Ratings.csv')
    courses_path=os.path.join(training_path,'CPD_Courses.csv')
    df_ratings = pd.read_csv(
    os.path.join(training_path,'Ratings.csv'),
    usecols=['UserID', 'CourseID', 'Rating'],
    dtype={'UserID': 'int32', 'CourseID': 'str', 'Rating': 'float32'})
    df_courses = pd.read_csv(courses_path)
    
    return df_ratings,df_courses

def get_list_of_categories(courses):
    category_list = []
    
    for category in courses.Categories.str.split('|'):
        for name in category:
            if name not in category_list: 
                category_list.append(name.strip())
            
    return category_list

def get_column_name_list(category_list):
    column_name = []
    
    for category in category_list:
        column_name.append('avg_' + category.strip() + '_watch')
      
    return column_name

#category watch time across ALL students across ALL categories
def get_all_category_watch_time(users, courses):
    category_ratings = pd.DataFrame(columns = ['UserID'])
    category_list = get_list_of_categories(courses)
    column_names = get_column_name_list(category_list)
    
    #add studentId to list of columns
    column_names.insert(0,'UserID')
    
    for category in category_list:        
        course_categories = courses[courses['Categories'].str.contains(category)]
        
        #determine the average watch time for the given category; retain the studentId
        avg_watch_time_per_user = users[users['CourseID'].isin(course_categories['CourseID'])].loc[:, ['UserID', 'ratings']].groupby(['UserID'])['ratings'].mean().round(2).reset_index()      
    
        #merge the ratings for the given catetgory with the prior categories
        category_ratings = category_ratings.merge(avg_watch_time_per_user, on='UserID', how='outer')
            
    category_ratings.columns = column_names
    return category_ratings

def dump(file_name, predictions, trainset=None, algo=None):
    """Dump a list of :obj:`predictions
    <surprise.prediction_algorithms.predictions.Prediction>` for future
    analysis, using Pickle.

    If needed, the :class:`trainset <surprise.dataset.Trainset>` object and the
    algorithm can also be dumped. What is dumped is a dictionnary with keys
    ``'predictions``, ``'trainset'``, and ``'algo'``.

    The dumped algorithm won't be a proper :class:`algorithm
    <surprise.prediction_algorithms.algo_base.AlgoBase>` object but simply a
    dictionnary with the algorithm attributes as keys-values (technically, the
    ``algo.__dict__`` attribute).

    See :ref:`User Guide <dumping>` for usage.

    Args:
        file_name(str): The name (with full path) specifying where to dump the
            predictions.

        predictions(list of :obj:`Prediction\
            <surprise.prediction_algorithms.predictions.Prediction>`): The
            predictions to dump.
        trainset(:class:`Trainset <surprise.dataset.Trainset>`, optional): The
            trainset to dump.
        algo(:class:`Algorithm\
            <surprise.prediction_algorithms.algo_base.AlgoBase>`, optional):
            algorithm to dump.
    """

    dump_obj = dict()

    dump_obj['predictions'] = predictions

    if trainset is not None:
        dump_obj['trainset'] = trainset

    if algo is not None:
        dump_obj['algo'] = algo.__dict__  # add algo attributes
        dump_obj['algo']['name'] = algo.__class__.__name__

    pickle.dump(dump_obj, open(file_name, 'wb'))
    print('The dump has been saved as file', file_name)

def train():
    print('Training job started')
    try:
        ratings,courses=load_data()
        
        popularity_thres = 30
        df_course_cnt = pd.DataFrame(ratings.groupby('CourseID').size(), columns=['count'])
        popular_courses = list(set(df_course_cnt.query('count >= @popularity_thres').index))
        print('Popular Courses:',len(popular_courses))
        df_ratings_drop_courses = ratings[ratings.CourseID.isin(popular_courses)]
        print('shape of original ratings data: ', ratings.shape)
        print('shape of ratings data after dropping unpopular courses: ', df_ratings_drop_courses.shape)
    
        df_users_cnt = pd.DataFrame(df_ratings_drop_courses.groupby('UserID').size(), columns=['count'])
        # filter data to come to an approximation of user likings.
        ratings_thres = 5
        active_users = list(set(df_users_cnt.query('count >= @ratings_thres').index))
        print('Active Users:',len(active_users))
        df_ratings_drop_users = df_ratings_drop_courses[df_ratings_drop_courses.UserID.isin(active_users)]
        print('shape of original ratings data: ', ratings.shape)
        print('shape of ratings data after dropping both unpopular courses and inactive users: ', df_ratings_drop_users.shape)

        # pivot and create course-user matrix
        course_user_mat = df_ratings_drop_users.pivot(index='CourseID', columns='UserID', values='Rating').fillna(0)

        # transform matrix to scipy sparse matrix
        course_user_mat_sparse = csr_matrix(course_user_mat.values)

        # define model
        model_knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=20, n_jobs=-1)
        # fit
        model_knn.fit(course_user_mat_sparse)
        # Save model
        print(f"Saving model in {model_path} ...")
        
        dump.dump(model_path, algo=model_knn, predictions=None)
        print("Training complete.")

    except Exception as e:
        # Write out an error file. This will be returned as the reason for
        # failure in the DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)


if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
