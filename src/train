#!/usr/bin/env python

import os
import sys
import traceback
import pickle
from sklearn.cluster import KMeans
import pandas as pd

# Paths where SageMaker mounts data in our container
prefix = '/opt/ml/'
input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model', 'recommender_system.csv')

# This algorithm has two channels: training (mandatory) and testing (optional).
# When input method is file mode, the input files are copied to the directory
# specified here.
channel_name_train = 'train'
training_path = os.path.join(input_path, channel_name_train)
""" channel_name_test = 'test'
test_path = os.path.join(input_path, channel_name_test)
test_flag = False
 """

progress_path = os.path.join(training_path, 'Ratings.csv')

def load_data():
    progress_path=os.path.join(training_path,'Progress.csv')
    courses_path=os.path.join(training_path,'CPD_Courses.csv')
    df_progress = pd.read_csv(progress_path)
    df_courses = pd.read_csv(courses_path)
    
    return df_progress,df_courses

def get_list_of_categories(courses):
    category_list = []
    
    for category in courses.Categories.str.split('|'):
        for name in category:
            if name not in category_list: 
                category_list.append(name.strip())
            
    return category_list

def get_column_name_list(category_list):
    column_name = []
    
    for category in category_list:
        column_name.append('avg_' + category.strip() + '_watch')
      
    return column_name

#category watch time across ALL students across ALL categories
def get_all_category_watch_time(users, courses):
    category_progress = pd.DataFrame(columns = ['UserID'])
    category_list = get_list_of_categories(courses)
    column_names = get_column_name_list(category_list)
    
    #add studentId to list of columns
    column_names.insert(0,'UserID')
    
    for category in category_list:        
        course_categories = courses[courses['Categories'].str.contains(category)]
        
        #determine the average watch time for the given category; retain the studentId
        avg_watch_time_per_user = users[users['CourseID'].isin(course_categories['CourseID'])].loc[:, ['UserID', 'Progress']].groupby(['UserID'])['Progress'].mean().round(2).reset_index()      
    
        #merge the progress for the given catetgory with the prior categories
        category_progress = category_progress.merge(avg_watch_time_per_user, on='UserID', how='outer')
            
    category_progress.columns = column_names
    return category_progress


def train():
    print('Training job started')
    try:
        progress,courses=load_data()
        
    
        #convert progress percentage string to numeric data
        progress['Progress'] = progress['Progress'].astype('float') / 100.0

        # Calculate the average rating all categories per user
        category_watch_time_df = get_all_category_watch_time(progress, courses)
        
        #replace NaN with 0
        category_watch_time_df = category_watch_time_df.fillna(0)
        
        #remove student id from dataframe
        category_watch_time_list = category_watch_time_df.drop(['UserID'], axis=1)
        
        category_watch_time_list.shape
        
        # Turn our dataset into a list
        category_watch_time_list = category_watch_time_list.values
        
        # Create an instance of KMeans to find 20 clusters
        km = KMeans(n_clusters=20, random_state=0)

        # Use fit_predict to cluster the dataset
        # Returns a cluster prediction for each student / ie cluster labels
        predictions = km.fit_predict(category_watch_time_list)

        
       
        #convert numpy array to dataframe and give column name of cluster
        cluster_df = pd.DataFrame(data=predictions)
        cluster_df.columns = ['assigned_cluster']
        
        
        # merge data to see the assigned cluster for user ID and drop unnecessary columns
        user_cluster_df = pd.DataFrame(columns = ['UserID', 'assigned_cluster'])
        user_cluster_df = pd.concat([cluster_df, category_watch_time_df], axis=1)
        user_cluster_df = user_cluster_df[user_cluster_df.columns[user_cluster_df.columns.isin(['UserID', 'assigned_cluster'])]]

        # Save model
        print(f"Saving model in {model_path} ...")
        
        pickle.dump(user_cluster_df, open(model_path, 'wb'))
        print("Training complete.")

    except Exception as e:
        # Write out an error file. This will be returned as the reason for
        # failure in the DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)


if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
